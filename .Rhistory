roxygen2::roxygenize()
?bootstrapping()
adult
adult <- read.csv("adult.csv", sep=";")
?bootstrapping
read.csv("adult.csv", sep=";")
adult <- read.csv("adult.csv", sep=";")
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult, B=5)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
#' @export
graph_ci = function(){
appDir = system.file("graph_ci", package = "FinalProject6")
shiny::runApp(appDir, display.mode = "normal")
}
#' @title Loss Function
#'
#' @description Define the loss function that we will use to optimize for
#' binary classification.
#' @param beta A \code{vector} containing the beta that we will evaluate the
#' loss function for.
#' @param x A \code{matrix} containing the formatted feature data.
#' @param y A \code{vector} containing the classifications of the features.
#' @return A \code{numeric} which is the log likelihood value.
#' @author Emily Knight
loss_function <- function(beta, x, y){
p <- 1 / (1 + exp(-x %*% beta))
# Check if any p is exactly 0 or 1, which would cause log(0) or log(1) errors
if (any(p == 0 | p == 1)) {
epsilon <- 1e-15
p <- pmax(pmin(p, 1 - epsilon), epsilon)
# Notify the user if epsilon was applied
if (!epsilon_applied) {
warning("Log(0) or Log(1) encountered. Epsilon was applied to probabilities to avoid numerical errors.")
epsilon_applied <<- TRUE
}
}
# Negative log-likelihood
log_likelihood <- -sum(y * log(p) + (1 - y) * log(1 - p))
return(log_likelihood)
}
#'           \item{beta_estimate}{A k+1 x 1 \code{vector} which is the optimal beta hat
#'           coefficient vector calculated by optimizing the provided loss function}
#'      }
#'    }
#' }
#' @author Emily Knight
#' @export
#' @examples
#' logistic_regression(x = c("age", "workclass", "house.per.week"),
#'          y = "sex", data = adult)
logistic_regression <- function(x, y, data){
list1 <- format_input(x = x, y = y, data = data)
# Determine Initial Beta Coefficients
beta_initial <- beta_initial_guess(x, y, data)
# set initial epsilon_applied to FALSE for use in generating a warning if used in the loss function
epsilon_applied <<- FALSE
# estimate coefficients
optimal_result <- optim(par = beta_initial,
fn = loss_function,
x = list1$x,
y = list1$y,
method = "BFGS")
beta_estimate <- optimal_result$par
# update list
list2 <- list(Coefficient = colnames(list1$x), beta_initial = beta_initial,  beta_estimate = beta_estimate)
my_output <- list(user_data = list1, coefficients = list2)
# provide user with relevant output
print(my_output$coefficients)
return(my_output)
}
# Prepare data for boxplot
coefficients <- colnames(beta_bootstrap_estimates)
#' of observations in the given data frame and k is the number of provided features
#' to predict on:
#' \describe{
#'      \item{x}{An n x k+1 matrix containing the predictors we will use to train}
#'      \item{y}{An n x 1 vector contatining the classification of each observation}
#' }
#' @author Emily Knight
#' @examples
#' format_input(x = c("age", "workclass", "house.per.week"),
#'       y = "sex", data = adult)
format_input <- function(x, y, data){
# check user input
## generate an appropriate error if the corresponding column names not found or if data is not a data frame
# Check if data is a data frame
if (!is.data.frame(data)) {stop("The input 'data' must be a data frame.")}
if(!y %in% colnames(data)){stop(paste("The y column of interest,", y, "was not found in the data frame of interest. Please double check the name of the column you are interested in and try again!"))}
if(!all(x %in% colnames(data))){stop(paste("One of more of the x columns you are interested in was not found in the data frame of interest."))}
## extract x and y columns from the data frame
y <- data[[y]]
x <- data[ , x, drop = FALSE]
## format
## ensure y is treated as numeric, since using numerical optimization for binary classification
## ensure x is formatted to numeric with character / factors being converted to dummy variables
y <- as.factor(y)
y <- as.numeric(y) - 1
x <- model.matrix(~ ., data = x)
## store current output in list
list1 <- list(x = x, y = y)
return(list1)
}
#'           \item{Accuracy}{}
#'           \item{Sensitivity}{}
#'           \item{Specificity}{}
#'           \item{FalseDiscoveryRate}{}
#'           \item{DiagnosticOddsRatio}{}
#'      }
#'      }
#'    }
#' @author Shuqi Du
#' @export
confusion_metrics <- function(beta, x, y) {
# Calculate predicted probabilities
p <- 1 / (1 + exp(-x %*% beta))
# Convert probabilities to binary predictions using a cutoff of 0.5
y_pred <- ifelse(p > 0.5, 1, 0)
# Create confusion matrix
tp <- sum(y_pred == 1 & y == 1)
tn <- sum(y_pred == 0 & y == 0)
fp <- sum(y_pred == 1 & y == 0)
fn <- sum(y_pred == 0 & y == 1)
confusion_matrix <- matrix(c(tp, fp, fn, tn), nrow = 2, byrow = TRUE)
rownames(confusion_matrix) <- c("Predicted 1", "Predicted 0")
colnames(confusion_matrix) <- c("Actual 1", "Actual 0")
# Calculate metrics
prevalence <- (tp + fn) / (tp + tn + fp + fn)
accuracy <- (tp + tn) / (tp + tn + fp + fn)
sensitivity <- tp / (tp + fn)
specificity <- tn / (tn + fp)
false_discovery_rate <- fp / (tp + fp)
diagnostic_odds_ratio <- (tp / fn) / (fp / tn)
metrics <- list(
Prevalence = prevalence,
Accuracy = accuracy,
Sensitivity = sensitivity,
Specificity = specificity,
FalseDiscoveryRate = false_discovery_rate,
DiagnosticOddsRatio = diagnostic_odds_ratio
)
return(list(ConfusionMatrix = confusion_matrix, Metrics = metrics))
}
#'      \item{ConfidenceIntervals}{A 3 x p matrix containing the confidence intervals for
#'      beta for each feature. The first row, Lower, contains the lower estimate; the second row,
#'      Estimate, contains the mean from the bootstrap procedure; the third row, Upper, contains
#'      the upper estimate.}
#' }
#' @author Emily Knight
#' @export
#' @examples
#' bootstrapping(x = c("age", "workclass", "hours.per.week"),
#'      y = "sex", data = adult, alpha = 0.1, B = 100)
bootstrapping <- function(x, y, data, alpha = 0.05, B = 20) {
## Format user data
list1 <- format_input(x, y, data)
x <- list1$x
y <- list1$y
# Determine Initial Beta Coefficients
beta_initial_guess2 <- function(x, y){
xx <- t(x)%*%x
# Check for singularity
if (det(xx) == 0) stop("Matrix X'X is singular and cannot be inverted.")
xx <- solve(xx)
return(xx %*% t(x) %*% y)
}
## Create matrices to store the results
beta_bootstrap_estimates <- matrix(NA, nrow = B, ncol = ncol(x)) # Store optimized coefficients
beta_bootstrap_initial <- matrix(NA, nrow = B, ncol = ncol(x))  # Store initial guesses
n <- nrow(data)
## Perform bootstrapping
for (i in 1:B) {
# Resample data
index <- sample(1:n, size = n, replace = TRUE)
x_boot <- x[index, , drop = FALSE]
y_boot <- y[index]
# Calculate initial beta for resampled data
beta_boot <- beta_initial_guess2(x = x_boot, y = y_boot)
beta_bootstrap_initial[i, ] <- beta_boot
# Optimize to find "optimal" beta
boot_optim <- optim(par = beta_boot,
fn = loss_function,
x = x_boot,
y = y_boot,
method = "BFGS")
beta_bootstrap_estimates[i, ] <- boot_optim$par
}
## Compute Confidence Intervals (Percentile Method)
lower_bound <- apply(beta_bootstrap_estimates, 2, function(est) quantile(est, probs = alpha / 2))
upper_bound <- apply(beta_bootstrap_estimates, 2, function(est) quantile(est, probs = 1 - alpha / 2))
estimate <- colMeans(beta_bootstrap_estimates) # Average bootstrap estimates
## Combine Results into Lists
coefficients_mat <- list(
Beta_Bootstrap_Estimates = beta_bootstrap_estimates,
Beta_Bootstrap_Initial = beta_bootstrap_initial
)
confidence_intervals_mat<- rbind(
Lower = lower_bound,
Estimate = estimate,
Upper = upper_bound
)
colnames(confidence_intervals_mat) <- colnames(x)
## Output
result <- list(
Coefficients = coefficients_mat,
ConfidenceIntervals = confidence_intervals_mat
)
print(confidence_intervals_mat)
return(result)
}
#' @param y A \code{character} containing the name of the classified feature
#' (which we will fit the model on).
#' @param data A \code{data frame} containing the data we will train the binary
#' classification on.
#' @return A p x 1 \code{vector} containing the initial beta coefficient values,
#' where p is the number of features given plus 1 for the intercept.
#' @author Emily Knight
#' @examples
#' beta_initial_guess(x = c("age", "workclass",
#'        "hours.per.week"), y = "sex", data = adult)
beta_initial_guess <- function(x, y, data){
## Format user data
list1 <- format_input(x, y, data)
x <- list1$x
y <- list1$y
xx <- t(x)%*%x
# Check for singularity
if (det(xx) == 0) stop("Matrix X'X is singular and cannot be inverted.")
xx <- solve(xx)
return(xx %*% t(x) %*% y)
}
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult, B=5)
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult, alpha = 0.5, B=5)
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
num_coefficients <- length(coefficients)
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,B=5)
# set initial epsilon_applied to FALSE for use in generating a warning if used in the loss function
epsilon_applied <<- FALSE
# set initial epsilon_applied to FALSE for use in generating a warning if used in the loss function
# set initial epsilon_applied to FALSE for use in generating a warning if used in the loss function
epsilon_applied <<- FALSE
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,B=5)
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,B=5)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,alpha=0.05,B=5)
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,alpha=0.05,B=5)
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex")
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
beta_bootstrap_estimates <- bootstrap_result$Coefficients$Beta_Bootstrap_Estimates[, , drop = FALSE]
graph_ci <- function(x, y, data, alpha = 0.05, B = 20) {
# Extract bootstrap estimates
bootstrap_result <- bootstrapping(x = x, y = y, data = data, alpha = alpha, B = B)
beta_bootstrap_estimates <- bootstrap_result$Coefficients$Beta_Bootstrap_Estimates[, , drop = FALSE]
# Prepare data for boxplot
coefficients <- colnames(beta_bootstrap_estimates)
num_coefficients <- length(coefficients)
# Set up the plot area
boxplot(
beta_bootstrap_estimates,
names = coefficients,
main = "Bootstrap Confidence Intervals for Coefficients",
xlab = "Coefficient",
ylab = "Beta Estimates",
las = 2,  # Rotate x-axis labels for readability
col = "lavender",
border = "navy"
)
# Add grid for better readability
grid(nx = NA, ny = NULL, col = "ghostwhite", lty = "dotted")
}
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult)
#' @param B A \code{integer} representing the number of of bootstraps to run. Default
#' set to 20.
#' @return A graph with p boxplots, where p is the number of features you ran the bootstrap
#' procedure on plus the intercept. The y axis shows the value for the beta feature, and the
#' x axis has the p boxplots.
#' @author Emily Knight
#' @export
#' @examples
#' graph_ci(x = c("age", "workclass", "hours.per.week"),
#'      y = "sex", data = adult, alpha = 0.1, B = 100)
graph_ci <- function(x, y, data, alpha = 0.05, B = 20) {
# Extract bootstrap estimates
bootstrap_result <- bootstrapping(x = x, y = y, data = data, alpha = alpha, B = B)
beta_bootstrap_estimates <- bootstrap_result$Coefficients$Beta_Bootstrap_Estimates[, , drop = FALSE]
# Prepare data for boxplot
coefficients <- colnames(beta_bootstrap_estimates)
num_coefficients <- length(coefficients)
# Set up the plot area
boxplot(
beta_bootstrap_estimates,
names = coefficients,
main = "Bootstrap Coefficient Estimate Boxplots",
xlab = "Coefficient",
ylab = "Beta Estimates",
las = 2,  # Rotate x-axis labels for readability
col = "lavender",
border = "navy"
)
# Add grid for better readability
grid(nx = NA, ny = NULL, col = "ghostwhite", lty = "dotted")
}
roxygen2::roxygenize()
rm(list = c("bootstrapping", "confusion_metrics", "graph_ci", "logistic_regression"))
roxygen2::roxygenize()
adult <- read.csv("Documents/GitHub/Final_Project_Group_6/adult.csv", sep=";")
adult <- read.csv("adult.csv", sep=";")
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,alpha=0.05,B=3)
bootstrapping(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,alpha=0.05,B=3)
#' @param B A \code{integer} representing the number of of bootstraps to run. Default
#' set to 20.
#' @return A graph with p boxplots, where p is the number of features you ran the bootstrap
#' procedure on plus the intercept. The y axis shows the value for the beta feature, and the
#' x axis has the p boxplots.
#' @author Emily Knight
#' @export
#' @examples
#' graph_ci(x = c("age", "workclass", "hours.per.week"),
#'      y = "sex", data = adult, alpha = 0.1, B = 100)
graph_ci <- function(x, y, data, alpha = 0.05, B = 20) {
# Extract bootstrap estimates
bootstrap_result <- bootstrapping(x, y, data, alpha, B)
beta_bootstrap_estimates <- bootstrap_result$Coefficients$Beta_Bootstrap_Estimates[, , drop = FALSE]
# Prepare data for boxplot
coefficients <- colnames(beta_bootstrap_estimates)
num_coefficients <- length(coefficients)
# Set up the plot area
boxplot(
beta_bootstrap_estimates,
names = coefficients,
main = "Bootstrap Coefficient Estimate Boxplots",
xlab = "Coefficient",
ylab = "Beta Estimates",
las = 2,  # Rotate x-axis labels for readability
col = "lavender",
border = "navy"
)
# Add grid for better readability
grid(nx = NA, ny = NULL, col = "ghostwhite", lty = "dotted")
}
graph_ci(x = c("age", "workclass", "hours.per.week"), y = "sex", data = adult,alpha=0.05,B=3)
